<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<title>Interaction in Virtual and Augmented Reality - Cr0ydon's Blog</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="My project for the university course Interaction in Virtual and Augmented Reality.">

			<link rel="icon" type="image/png" href="../../../assets/general/icon/icon.png">
	
  	<!-- Social media integration -->
	<meta name="twitter:title" content="Interaction in Virtual and Augmented Reality">
	<meta name="twitter:image" content="https://blog.cr0ydon.com/assets/2024/ivar/thumbnail.jpg">
	<meta name="twitter:url" content="https://blog.cr0ydon.com/2024/04/interaction-in-virtual-and-augmented-reality-project/">
	<meta name="twitter:card" content="summary_large_image">
	<meta property="og:title" content="Interaction in Virtual and Augmented Reality">
	<meta property="og:image" content="https://blog.cr0ydon.com/assets/2024/ivar/thumbnail.jpg">
	<meta property="og:url" content="https://blog.cr0ydon.com/2024/04/interaction-in-virtual-and-augmented-reality-project/">
	<meta property="og:site_name" content="Cr0ydon's Blog">
	<meta property="og:description" content="My project for the university course Interaction in Virtual and Augmented Reality.">

	<!-- Animate.css -->
	<link rel="stylesheet" href="../../../themes/historia/css/animate.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="../../../themes/historia/css/bootstrap.css">
	<link rel="stylesheet" href="../../../themes/historia/css/bootstrap-theme.css">
	<link rel="stylesheet" href="../../../themes/historia/css/bootstrap-icons.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="../../../themes/historia/css/style.css">

	</head>
	<body class="single">
		
	<div class="historia-loader"></div>
	
	<div id="page">
		<div id="fh5co-aside" style="background-image: url(../../../assets/2024/ivar/thumbnail.jpg)" data-stellar-background-ratio="0.5">
			<div class="overlay"></div>
			<nav role="navigation">
				<ul>
					<li><a href="../../../"><i class="bi bi-house-door-fill" aria-label="Homepage"></i></a></li>
									<li><a href="../../../links"><i class="bi bi-person-lines-fill" aria-label="Links"></i></a></li>
								</ul>
			</nav>
			<div class="page-title">
				<span>01 April 2024</span>
				<h2>Interaction in Virtual and Augmented Reality</h2>
			</div>
		</div>
		<div id="fh5co-main-content">
			<div class="fh5co-post"> 
				<div class="fh5co-entry padding">
					<div>
						<p>[Off-Topic note: The view of this blog post is not (yet) optimized for mobile browsers, I would recommend to read it on a bigger screen if possible.]</p>
<p>This is my report for my university project for the course Interaction in Virtual and Augmented Reality.</p>
<h2>Motivation</h2>
<p>We have been given an <a href="https://github.com/wenjietseng/VR-locomotion-parkour">existing VR project</a>. In this virtual environment exists a race track through a city. Along the way are collectable coins. The race track is structured in three different areas. Shortly before a new area, a task has to be done, at which a T-shaped object has to be selected, rotated and moved in the second T-shaped object. Ideally, those two T-objects are then exactly at the same position with the same rotation, so that you can't distingish them anymore.</p>
<p>We had the following tasks:</p>
<ul>
<li>Implement a locomotion method</li>
<li>Implement a method that allows selection, moving and rotation of the T-objects</li>
<li>Implement something that reduces cyber-sickness</li>
<li>Design and execute a study to evalute the results</li>
</ul>
<h2>Initial Implementation Idea</h2>
<p>My initial idea was to implement some flying. Either some Ironman-like flying locomotion where you put your open hands down horizontally and it applies some upwards force and some force in the direction of where you are looking. Or some Superman-like flying where you have some power-pose that let you fly in the direction of your strechted arm, both horizontal and vertical.</p>
<p>For the interaction task, I wanted to stay in this superhero-themed setting and either use some Ironman-like energy beams that attach to the T-object on the first hit and then rotates and moves with your hand. Or effectively the same thing with some Spiderman webs.</p>
<p>For the cyber-sickness reduction, I planned to implement a tunnling vignette.</p>
<h2>Technical Details and Actual Implementation</h2>
<h3>General</h3>
<p>The implementation is build with Unity and requires a Meta Quest 2 headset to run. </p>
<p>The original implementation used Unity 2021.3.10f1 and the Oculus Integration 46.0 package. Over the timespan of my development, I first updated to the latest Unity patch version and later performed an upgrade to Unity 2022.3.19f1. First of all, I wanted to be able to use all currently existing engine features and packages, that only work with newer Unity versions, and secondly, even more important, I also had another university project at the same time, that was also implemented in this Unity version. I did not want to learn and work with two different Unity version simultaneously, particular as I keep getting the impression, that Unity is changing things regularly in a backwards incompatible way.</p>
<p>Upgrading Unity to the 2022.3 LTS version also forced me to migrate from the now legacy Oculus integration, that was directly available from the Unity package registry, to the new Meta SDKs, that are either available from the Unity Asset Store or directly via Meta's npm registry. (Side note: The Unity package manager builds upon npm. Therefore other npm registries can be added and then directly used inside the Unity Editor's package manager.) I first went with the Asset Store version as I was already used to its integration workflow as I am also using other assets from there, but I had to switch to Meta's npm registry later on, when I added continuous integration (CI) and continuous delivery (CD) to my git repository, since the Asset Store always requires authentication with an Unity account, even for free assets, and my CI system does not support this for the Asset Store at the moment.</p>
<p>As part of the modernization of Meta's integration, I also switched from the existing, standard game objects that have Meta scripts attached, to <a href="https://developer.oculus.com/blog/accelerate-development-mixed-reality-building-blocks-unity-meta-quest-developers/">Meta SDKs' building blocks</a>.</p>
<p>Unity itself has several official packages for XR development. I experimented around with several of them, but ultimately do not use them in my final state of the project. Those XR packages do not seem to be compatible with Meta SDKs, therefore would have required me to re-implement everything and re-learn a lot. For this, the time was to limited. However, I have the impression that Unity's XR packages are probably a solid choice for any new project, for several reason:</p>
<ul>
<li>They work with a wider range of XR hardware, not just Meta hardware.</li>
<li>They come with a lot of ready-to-use functionality for interactions, locomotions and cyber-sickness reduction methods I would have been interested in, that in part do not exist at all or in that specific way in the Meta SDKs.</li>
<li>My subjective, perhabs misleading, impression is also, that there seems to be an overall trend towards those Unity packages. I oftentimes found more up-to-date information how to do X, Y, Z with those integrations, but had a much harder time to find the same information for the Meta SDKs.</li>
</ul>
<p>At the end, I also enabled Meta Quest 3 as a build target, however, this is completely untested and my project might not work at all with Quest 3 hardware.</p>
<h3>Game Loop Logic</h3>
<p>I made several changes and enhancements to the game loop logic. As my study design has the task to get as many rounds done in 10 minutes as possible I had to change the logic that previously ended the parkour after a single round. Now, the logic resets everything so that an endless amount of rounds are doable within the time limit. All coins, even previously collected ones, do re-spawn.</p>
<p>Originally, the interaction task was completely ignoreable as people could just walk (or fly) past it. I added a road blockade until all 5 T-objects for each area are done. The blockade also displays the number of objects that have yet to be positioned. Furthermore, when the appearance of the task is triggered, all locomotion techniques are disabled for the duration of the task, preventing (most) escape and cheat attempts.</p>
<p>I also changed many implementation details, including the logging for the study data.</p>
<h3>Locomotion</h3>
<p>I kept the original idea of enabling the player to fly, but eventually gave up on trying to implement a flying method that feels "natural". "Natural" meaning, that there is some mechanism that adds some amount of vertical force on the player and the player only slows down through gravity, crashing into another object or through some kind of break mechanism.</p>
<p>Getting the physics right for a scenario, where players have to constantly and quickly change directions is hard. I went through many iterations of differnt kinds of force-logic, some can be found in my git history, many others did not even make it so far. Most of them were really hard to control from a player perspective or caused very strong cyber-sickerness symptoms.</p>
<p>In one of those intermediate interactions, I had added a flying force by pressing the right trigger button. The player would than naturally fall down with gravity again. I eventually figured our physics numbers that made it controllable rather well, but I did not want this to be the final flying method, as it did not feel creative and innovative enough. It is still possible to turn this control on in my final solution, but it is off by default and can only be changed before a build, not during runtime.</p>
<p>In another one of those intermediate interations, I experimented with using the body height of the player to control the flying height, or to be more preceise, the height of the headset in relation to the floor height. Standing straight would mean having the maximum flying height, getting closer to a pre-determined min-height would bring down the player more and more. The max and min height need to be determined before the game begins in order for such a logic to work. For the max value to be correct, the game has to be sure that the player is standing straight, at the time of taking this value. And a min value has to be recorded in a moment, that the player is aware of too. One might think, that the floor can be taken as such a reference point, but that wouldn't be good either:</p>
<ul>
<li>It would be a terrible idea that the player has to bring their head down on the floor to reach the min flying height.</li>
<li>Different people have a different comfort level of how far they can bend their knees, not once, but often and for a longer time.</li>
</ul>
<p>It turned out, that the to be expected height difference of the headset, between standing straight and going down as far as it is comfortable and holdable for a longer time period, is too small for this use case. Even slight height chances would change the virtual player position dramatically. Therefore this method did not allow a precise enough control.</p>
<p>However, my final solution build upon this concept. Instead of the height of the headset in relation to the floor, I used the right controller height in relation to the virtual player object. Now, the implementation requires the player at the start to put up their right hand as far as comfortable and press a specific button. Then, they have to put their right hand down as far as comfortable and press another button. These min and max heights are then mapped to specific vertical forces. Within this height range:</p>
<ul>
<li>For the bottom &lt; 45% height positions, the player gets added a vertical force, that pulls them down. The lower the position, the quicker they will fall down, if the are already in the air flying.</li>
<li>For 45% until 56% (excluding) height positions, the player gets removed (almost) all vertical forces. This causes an "elevator flight" effect and enables the player to (almost) stand still in the air. (There is still a very tiny force downwards, as I have technically not disabled gravity for the player object, however, that is only noticeable at all, if there is no other velocity and if you really pay attention. For all practical purposes within the parkour it is irrelvant.)</li>
<li>For positions at 56% height and above, the player gets added a vertical force upwards. The force added is bigger the higher the position is.</li>
</ul>
<p>This "elevator flight" has assumingly a big enough range to control the flying precisely enough and fast enough for our race track. On the other hand, it was also the reason why I gave upon the superhero-theming alltogether.</p>
<p>For the vertical movement, I rather kept it simple: By pressing the left trigger button a certain force is getting added in the direction in which the player looks. The longer the button is pressed the more force is building up, allowing acceleration and giving a "drifting-feel". However, this lead to the problem that sudden changes of directions are really hard to control. In my final solution, I did not just leave it to gravity and player-added counter-forces to let the player slow down, but implemented a logic, that artificial reduces linearly the horizontal velocity to zero over a few seconds, that start as soon as the player is not adding new horizontal speed and stops when they are adding new horizontal speed.</p>
<h3>Cyber-Sickness Reduction</h3>
<p>For the tunneling vignette, I tried to use several pre-existing implementations, including:</p>
<ul>
<li><a href="https://github.com/sigtrapgames/VrTunnellingPro-Unity">Vr Tunneling Pro</a></li>
<li><a href="https://github.com/angsamuel/GingerVR/">GingerVR</a></li>
<li>Meta SDK vignette</li>
<li>Unity XR vignette</li>
</ul>
<p>The first two solutions did probably not work due to incompatibilities with my used Unity version. Meta's vignette was surprisingly undocumented and seems to be only directly mentioned in older Meta package versions. So perhabs this is somewhat unmaintained, but one way or another, I couldn't get it working. And finally, the Unity XR vignette is not compatible with the Meta SDKs as it requires to be attached to a singular camera object. Meta's integration has one camera game object for the left lens and one for the right lens.</p>
<p>Ultimately, I am succesfully using the <a href="https://github.com/ExtendRealityLtd/Tilia.Visuals.Vignette.Unity">Tilia.Visuals.Vignette.Unity</a> package. The vignette gets bigger/smaller/disabled dynamically, based on the player's overall velocity, both horizontal and vertical.</p>
<p><img src="../../../assets/2024/ivar/blockade_and_vignette.jpg" alt="An image from inside VR, that shows both the road blockade before a new area of the race track and the vignette that appears when the player moves quickly." title="An image from inside VR, that shows both the road blockade before a new area of the race track and the vignette that appears when the player moves quickly." /></p>
<h3>Interaction</h3>
<p>Since I moved away from the superhero-theming, my original idea for the interaction task did not fit anymore. I searched for a new thematical inspiration and found one in the game Portal 2.</p>
<p>The original parkour project enabled to grab and rotate the T-objects by moving the controllers inside the T-objects, pressing the grab button and than control it with regular hand and arm movements.</p>
<p>This has several advantages:</p>
<ul>
<li>6 degrees of freedom</li>
<li>rather intuitive and natural</li>
<li>direct way to manipulate</li>
</ul>
<p>But it also means, that you are limited by your arm lenght and physical environment to reach things.</p>
<p>I kept the general idea of using this kind of direct, controller-based interaction, but advanced it by adding portals. Now, when the object task is triggered, a blue entry portal appears directly in front of the player. On the left border side of this portal, a "start" button appears. Once the players touches it, an orange exit portal appears together with the moveable T-object and the target T-object. As soon as the players moves one or both physical controller through the blue entry portal, the virtual controllers are getting teleported to the orange exit portal. Being there, they can generally still be controlled as always. However, when the exit portal is not parallel to the entry portal, there is some mental re-thinking required as the rotation of the controllers is different than their physical counterparts.</p>
<p>I also experimented with exit portals that have their front side inverse to the entry portal (basically a rotation of ~180 degree), but this was way too hard to control. Therefore such exit portals are not part of the final parkour. Within certain ranges, the exit portal and the T-objects' positions and rotations are randomized, causing sometimes easier interaction tasks and sometimes tough ones.</p>
<p>Whenever the player thinks that they are done with placing one T-object, they can hit a "done" button on the right side of the entry portal. After a short cool off, the "start" button re-appears and has to be hit again for the next exit portal and T-objects to appear.</p>
<p>One of the positive effects of this portal interaction method is, that objects can be reached that normally could not be reached with the given physical limitations. To conclude, I ended up with the same advantage as my original interaction idea would have had, but also finished with a method, that still feels more natural than shooting spider webs or some energy beam.</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/aYS97-tJnGo?si=ZB0KaX2TCxXjxyqd" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<p><br></p>
<h2>Unity Quirks and Iteration Time</h2>
<p>While Unity is a powerful tool with many features, it has also many quirks that, at times, were really frustrating.</p>
<p>My biggest pain point with Unity are the incompatibilities everywhere. Not just between different version of the Unity Editor itself, but also between different versions of official(!) Unity packages. And even within the same Unity Editor version with the same sets of packages and package version, you can find many project-based toggles than switch between some old implementations and behaviours of <em>something</em>. Additionally, you have many fundamental component choices like which Unity <a href="https://docs.unity3d.com/Manual/render-pipelines.html">render pipeline</a> your project uses, that have signifiant influrence on what you can do in the first place. And changing later might require re-working major parts of a project, which is oftentimes not feasible.</p>
<p>All of those things, lead to the phenomenon that whenever you are searching for learning resources, packages, scripts, assets etc. there is an enourmously high chance, that it won't work for your project for one reason or another. This does not apply to basics, so while learning the basics of Unity, you should be fine. Unity experts are probably also not that much effected by that, as they have enough knowledge to workaround this or implement everything by themselves in the first place. But for everyone falling in-between, this is frustrating.</p>
<p>Moreover, I encountered some things, that might or might not be (directly) Unity faults. For some time, I had my Unity project saved on an external drive instead of an internal one. This caused Unity several times to crash for no apparent reasons. Somehow this crash managed to also disconnect all external devices of my computer for a while, before they automatically came back. One time, the crash even corrupted my git repository that contains the Unity project. Thankfully, I could clone my work from GitHub and had lost very little work. Those Unity crashes stopped instantly when I moved the project to an internal drive. And just for the protocol, I have all my other git project on this external drive and never encountered problems.</p>
<p>Another annoying obstacles has been the iteration time for each (change -&gt; build -&gt; test) cyclus. Whenever I changed a script of my project, the Unity Editor starts to re-import all Assets (scripts count as assets in Unity's terminology), leaving me with a progress bar that takes one to two minutes. Only after that, I can even trigger a new build. If you are doing this for a few months, it becomes almost like a trance where you oftentimes don't even know anymore if you waited for the first process to finish or the second one. Unity lacks a clear indication if there were any project changes after the last build.</p>
<p>The Meta integration (and the former Oculus integration) provide another option to build the project for a Meta Quest device, besides the default Unity build options. Those Meta build dialogs promise to be quicker, since they are using better caching. For my project it turns out, that those builds took ~ 12 - 14 minutes for each build, while the default Unity build option took around 8 - 9 minutes, at the time when I started to use the default build. It is now even faster, since I removed a lot of unused assets from the project and applied some more recommended settings (e.g. <a href="https://blog.unity.com/engine-platform/better-build-times-and-iteration-speed-for-quest">disabling strip engine code</a>, <a href="https://github.com/Croydon/tuda-vr-parkour/commit/473850f2f4146e91d7c7cc7c50a0de7b05028dd5">commit</a>).</p>
<h2>CI / CD</h2>
<p>I also added CI and CD for my project via GitHub Actions. For each git push, the Unity projects gets build, any warnings and errors get reported and if the build is successful, the APK file is getting uploaded to GitHub as a build artifact. When I tag a new version of my implementation, a GitHub Release gets created and the APK file is getting uploaded to it as well.</p>
<p>You can find the <a href="https://github.com/Croydon/tuda-vr-parkour/actions">CI runs  here</a> and the <a href="https://github.com/Croydon/tuda-vr-parkour/releases">releases here</a>.</p>
<h2>Study</h2>
<p>I designed an user study, made some early test run with a pre-study and then, with some small study adjustments and a heavily changed implementation, I performed the actual study.</p>
<h3>Study Design</h3>
<p>The pre-study and study design and their purpose do not match the typical structures of pre-studies and (actual) studies to some degree. Typically, you want to choose between a between-subject and a within-subject study structure and use the study to generate data, that helps to evalute the different levels of the independent variables of the study. In our case, we have three independent variables. Those are the locomotion system, the interaction system and the cyber-sickness reduction technique.</p>
<p>Due to the structure of the IVAR course, we basically only had to have one level of each independent variable, meaning only one implementation of each system / technique. We would then compare our results to the results of the other course students, that have their own implementations. This comparision is rather vague, given our freedom in the implementation, reaching from only changing the position of some coins to re-doing the entire virtual environment from the ground up in another game engine.</p>
<p>I extended this task a bit to come closer to a typical study structure again, by performing two user studies with a different implementation / level of all three systems / independent variables. I am going to call the first one pre-study, and the second just study or final study for the purpose of this report, even though the term <code>pre-study</code> is commonly used differently in most aspects.</p>
<p>The study is therefore a between-subject in-lab design, with two levels of each independent variable. Even though it is not a clean comparision, due to the fact that other conditions as the independent variables changed, which should not be the case ideally.</p>
<p>The general procedure of the user study is as follows:</p>
<ul>
<li>Welcome / Introduction</li>
<li>Ethnical disclaimer (participants are voluntary, unpaid, can end the experiment at any given time)</li>
<li>Privacy disclaimer (what data is saved and published?)</li>
<li>Pre-Questionnaire</li>
<li>Teaching / Explanation of the hardware, virtual environment, tasks and control</li>
<li>Participants can try the controls, locomotion and and interaction for some time</li>
<li>Explain the task once more: As many rounds as possible, as many collected coins as possible, as precise at the interaction task as possible. Your own choice where your priorities are.</li>
<li>10 minute task performance</li>
<li>Post-Questionnaire</li>
<li>Goodbye / Hardware cleaning</li>
</ul>
<p>You can find checklists that I made for myself to execute this study with participants in the study data repository (<a href="https://github.com/Croydon/tuda-vr-parkour-study/blob/main/pre_study/checklist_for_instructor_to_execute_pre_study.md">pre-study checklist</a>, <a href="https://github.com/Croydon/tuda-vr-parkour-study/blob/main/study/checklist_for_instructor_to_execute_post_study.md">final study checklist</a>). Such notes ensure, that every participant hears all explanations and experience all trainings and tasks, in the same order and in the same way. Avoiding accidental manipulation of the study results.</p>
<p>During the entire study, I make notes of everything interesting the participant might do and say (<a href="https://github.com/Croydon/tuda-vr-parkour-study/tree/main/pre_study/observations">pre-study</a>, <a href="https://github.com/Croydon/tuda-vr-parkour-study/tree/main/study/observations">final study</a>). Furthermore, there is automatical logging of (<a href="https://github.com/Croydon/tuda-vr-parkour-study/tree/main/pre_study/logs">pre-study</a>, <a href="https://github.com/Croydon/tuda-vr-parkour-study/tree/main/study/logs">final study</a>):</p>
<ul>
<li>How many coins were collected in which area and round.</li>
<li>How much time the participant took for each area, for moving through it and for the interaction task.</li>
<li>How precise the participant was for each object interaction.</li>
<li>How fast the participant was for each object interaction.</li>
<li>Some more logging for position and vectors, but those are mostly debug information that shouldn't matter for the actual study.</li>
</ul>
<p>I made some improvements to the study execution and the automatic logging after the pre-study, but my task instruction and the pre- and post-questionnaire were identical for both. You can find the full questionnaires here:</p>
<ul>
<li><a href="https://github.com/Croydon/tuda-vr-parkour-study/blob/main/study/pre_questionnaire/pre-questionnaire.md">Pre-Questionnaire</a></li>
<li><a href="https://github.com/Croydon/tuda-vr-parkour-study/blob/main/study/post_questionnaire/post-questionnaire.md">Post-Questionnaire</a></li>
</ul>
<p>For the final study, the view inside the VR headset was also video recorded for every participant. You can find a playlist of all <a href="https://www.youtube.com/playlist?list=PL2N-uWvcQaWtL2H0RFa1sC37JADNsBbkO">participant task videos on YouTube</a>.</p>
<p>Here is a video of myself doing the 10 minute task:</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/WJ9H1r-tjjA?si=LRXK7RNcCNR2oxgA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<p><br></p>
<h3>Study Results</h3>
<p>Due to time constraints, I could only perform the pre-study and actual study with a small mount of people.</p>
<p>The raw data of all results can be found in the <a href="https://github.com/Croydon/tuda-vr-parkour-study">study data repository</a>.</p>
<p>For the pre-study, participants were given an implementation that had a very different locomotion method, no cyber-sickness reduction method and the interaction was done directly by the controllers, without the portals or other more complex things. Furthermore, in the pre-study, they only had to do one interaction trask per race track part, instead of five.</p>
<p>Hence, the data between the pre-study and the final study is only compareable with some caution, unfortunately.</p>
<h4>Pre-Study Participants</h4>
<p>I had two participants (P1, P2), both male, being 29 and 27 years old (average: 28), that have rated their pre-existing experience on a scale from <code>1 - very inexperiened</code> to <code>5 - very experienced</code>, with a <code>2</code> and <code>1</code> respectively.</p>
<h4>Study Participants</h4>
<p>For the final study, I had four participants (P3 - P6), that were given the final implementation of the locomation and interaction method and with a enabled tunneling vignette.</p>
<p>One participant was female (P3), the others (P4 - P6) were male. The age average was 41.25 years (56, 50, 32, 27). On a scale from <code>1</code> to <code>5</code>, P5 estimated their previous VR experience with a <code>2</code>, while everyone else answered with a <code>1</code>.</p>
<h4>Discussion</h4>
<h5>Race Track Parts Finished</h5>
<table>
<thead>
<tr>
<th></th>
<th>Part1</th>
<th>Part 2</th>
<th>Part 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>P1</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>P2</td>
<td>5</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td><strong>Average pre-study</strong></td>
<td>4.5</td>
<td>4</td>
<td>3.5</td>
</tr>
<tr>
<td>P3</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>P4</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>P5</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>P6</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td><strong>Average study</strong></td>
<td>1.25</td>
<td>1.25</td>
<td>0.75</td>
</tr>
<tr>
<td><strong>Average total</strong></td>
<td>2.33</td>
<td>2.17</td>
<td>1.67</td>
</tr>
</tbody>
</table>
<p><br></p>
<h5>Collected Coins Per Part Average</h5>
<table>
<thead>
<tr>
<th></th>
<th>Part1 / max 16</th>
<th>Part 2 / max 30</th>
<th>Part 3 / max 23</th>
</tr>
</thead>
<tbody>
<tr>
<td>P1</td>
<td>0.75</td>
<td>20.33</td>
<td>15.33</td>
</tr>
<tr>
<td>P2</td>
<td>13.4</td>
<td>29</td>
<td>17</td>
</tr>
<tr>
<td><strong>Average pre-study</strong></td>
<td>7.08</td>
<td>24.67</td>
<td>16.17</td>
</tr>
<tr>
<td>P3</td>
<td>10</td>
<td>20</td>
<td>/</td>
</tr>
<tr>
<td>P4</td>
<td>11</td>
<td>14</td>
<td>/</td>
</tr>
<tr>
<td>P5</td>
<td>14</td>
<td>30</td>
<td>23</td>
</tr>
<tr>
<td>P6</td>
<td>9.5</td>
<td>12.5</td>
<td>16.5</td>
</tr>
<tr>
<td><strong>Average study</strong></td>
<td>11.13</td>
<td>19.13</td>
<td>19.75</td>
</tr>
<tr>
<td><strong>Average total</strong></td>
<td>9.78</td>
<td>20.97</td>
<td>17.96</td>
</tr>
</tbody>
</table>
<p>In the pre-study, the participants were able to finish more rounds. While this could correlate with the locomotion system theoretically, it is more plausible, that this due to the increased interaction requirement from solving only one object task in the pre-study per race track part, instead of five in the final study. Thus keeping the participants longer busy with the interaction task and preventing them from moving forward.</p>
<p>In the pre-study, only Part 3 had coins up in the air. In Part 1 and 2 all coins were placed on the ground in the pre-study, while the final study, had for every part some coins on the ground and some in the air. The results shows, that people have collected some more coins on average in the final study in Part 3 than in the pre-study, backing up my overall impression, that people struggled with jumping in the pre-study and had an easier time to collect them in the final study due to the new flying locomotion technique.</p>
<p>However, there is not enough data, to have a strong guess, why the average collected coins in part 1 and part 2 have mixed results. It might be, that people performed better in part 1 in the final study than in the pre-study, due to the locomotion system slowing down horizontal accerleration more quickly, thus allowing better control in this part of the race track with the most curves. In part 2, however, people collected more coins in the pre-study, this could be due to the fact, that all coins were on the ground in the pre-study, but many coins were flying in the final study - and simultanously, this part only has a single curve, thus, potential improvements in the horizontal agility might not have a huge impact, in contrast to the results of part 1.</p>
<h5>Task Interaction Error Offset Average</h5>
<p>Rounded to max. eight decimal places.</p>
<table>
<thead>
<tr>
<th></th>
<th>Part 1 Average</th>
<th>Part 2 Average</th>
<th>Part 3 Average</th>
<th>Total Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>P1</td>
<td>0.0005</td>
<td>0.00666667</td>
<td>0.00666667</td>
<td>0.006</td>
</tr>
<tr>
<td>P2</td>
<td>0.0052</td>
<td>0.002</td>
<td>0.005</td>
<td>0.02071428</td>
</tr>
<tr>
<td><strong>Average pre-study</strong></td>
<td>0.00285</td>
<td>0.00433334</td>
<td>0.005833335</td>
<td>0.01635714</td>
</tr>
<tr>
<td>P3</td>
<td>0.22259461</td>
<td>0.13199556</td>
<td>0.1298599</td>
<td>0.16148336</td>
</tr>
<tr>
<td>P4</td>
<td>0.13538194</td>
<td>0.24925465</td>
<td>0.06607917 (only 3 out of 5 finished)</td>
<td>0.16318619</td>
</tr>
<tr>
<td>P5</td>
<td>0.17476597  (in the second round, the time run out while working on the first object)</td>
<td>0.41176774</td>
<td>0.10948232</td>
<td>0.2284278825</td>
</tr>
<tr>
<td>P6</td>
<td>0.05867392</td>
<td>0.06364694</td>
<td>0.06099468</td>
<td>0.06110518</td>
</tr>
<tr>
<td><strong>Average study</strong></td>
<td>0.14785411</td>
<td>0.19062511</td>
<td>0.09160402</td>
<td>0.14336108</td>
</tr>
</tbody>
</table>
<p>The error offsets describes how far off the placed T-object was compared to the position of the target T-object. First, an error vector is calculated, one offset for the x, y and z coordinate axis respectively. Then, the magnitude of these vectors get calculated.</p>
<p>It is unfortunate, that the error offset in the pre-study was rounded to only two decimal places and the error vector was not logged. In the final study, this was improved to log exact values and additionally the error vector.</p>
<p>It was to be expected, that the precision for the tasks are better in the pre-study than in the final study. The direct manipulation of the objects with the controller, while standing directly in front of it, allows better precision, than having to look from a distance, hence the pre-study precision was on average 7.8 times better. Future work, should experiment with portals, that let you see the camera perspective of the exit portal inside the entry portal, thus eliminating the requirement to look from a distance. Furthermore, this would allow to have objects between the entry and exit portals that can block the view entirely.</p>
<p>P5 has a noteable spike in the error offset for the part 2 average, both globally and when comparing with P5's part 1 and part 3 average. This can be explained with the observation notes. P5 got once a portal and T-object spawn, that they found so hard, that the task was not even tried. To quote: "What is that? I can't see anything there. I can't reach that. Can I skip that? Yes, I am going to that now."</p>
<p>It it worth noting, that while this particular task spawn was indeed difficult, it was not impossible to solve (all possible spawns should be solveable). The exit portal was on the far right and very close to the entry portal, hence, the borders of both the entry and exit portals blocked a large portion of the view on the T-objects. However, leaning to the left and right allows to see enough of the T-objects theoretically. P6 had a similar verbal reaction to a difficult task spawn once, but had the patience to try to solve it and therefore has no notable spike in their error offsets.</p>
<h5>Cyber-Sickness, Presence, Enjoyment</h5>
<ul>
<li><strong>Cyber-Sickness</strong>: On a scale from 1 to 10, how much motion sickness do you perceive right now? On a scale from <code>1 - very low</code> to <code>10 - very high</code>.</li>
<li><strong>Presence</strong>: On a scale from 1 to 10 how present did you feel in the virtual world? On a scale from <code>1 - very low</code> to <code>10 - very high</code>.</li>
<li><strong>Enjoyment</strong>: On a scale from 1 to 10, how much fun did you have during the task? On a scale from <code>1 - very low</code> to <code>10 - very high</code>.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Cyber-Sickness</th>
<th>Presence</th>
<th>Enjoyment</th>
</tr>
</thead>
<tbody>
<tr>
<td>P1</td>
<td>2</td>
<td>6</td>
<td>6</td>
</tr>
<tr>
<td>P2</td>
<td>2</td>
<td>8</td>
<td>7</td>
</tr>
<tr>
<td><strong>Average pre-study</strong></td>
<td>2</td>
<td>7</td>
<td>6.5</td>
</tr>
<tr>
<td>P3</td>
<td>1</td>
<td>10</td>
<td>9</td>
</tr>
<tr>
<td>P4</td>
<td>6</td>
<td>7</td>
<td>8</td>
</tr>
<tr>
<td>P5</td>
<td>3</td>
<td>6</td>
<td>7</td>
</tr>
<tr>
<td>P6</td>
<td>1</td>
<td>6</td>
<td>5</td>
</tr>
<tr>
<td><strong>Average study</strong></td>
<td>2.75</td>
<td>7.25</td>
<td>7.25</td>
</tr>
<tr>
<td><strong>Average total</strong></td>
<td>2.5</td>
<td>7.17</td>
<td>7</td>
</tr>
</tbody>
</table>
<p>There seems to be no major difference on average when it comes to cyber-sickness symptoms, between the locomotion system of the pre-study and the final study. P4 has a spike, but individual people have different sensitivity to cyber-sickness. That there is no major difference on average is an interesting result, since the pre-study had no tunneling vignette, while the final study had one. Thus, one could expect, that the cyber-sickness symptoms in the pre-study are more severe. On the other hand, the pre-study locomotion system was based on a continious teleportion of the player. Teleportion is known to cause less cyber-sickness in general. The final study's locomotion system did use an accerlation system instead. Once more, more data would be required to have deeper insights.</p>
<p>The average response for the presence is almost identical (<code>7</code> in the pre-study, vs <code>7.25</code>). The average value for enjoyment has a small difference (<code>6.5</code> in the pre-study vs <code>7.25</code>), but there is too little data to make an educated guess why this might be.</p>
<h2>Future Work</h2>
<p>In the future, the study could be performed with more people to get more significant data. Furhermore, it should be performed with even more locomotions and interactions methods, to have more data to compare it to other implementations and to find out which locomotion and interaction systems allow people to be the fastest and most preceise and how cyber-sickness symptoms can be reduced at best. It should also be ensured, that all other conditions are the exact same to have cleaner data.</p>
<p>Several ideas did not made it into the final version of my project due to the limited time. For once, it would be nice to implement a restarting option directly into the game. Currently to start over, you have to close and re-open the application.</p>
<p>Second, I would have liked to experiment with portals, that allow you to see the camera perspective of the exit portal within the entry portal, thus eleminating the factor, that you have to look at the T-objects from afar, enabling that other objects can be in-between the entry and exit portal that block the natural line-of-sight and making a literal limitless range between the entry and exit portal possible. The precision of such a system might than be virtual identical to the results of the pre-study, that used direct controller grabbing manipulations.</p>
<p>Futhermore, I wanted thematically to go deeper into the Portal 2 direction. I began to create a prototype for a starting area in which the player spawns, when loading the application. While my concept also used this place for some entertainment, it also would have fulfilled the practical purpose of on-boarding the player. It would have explained how to configure the flying height with the right controller and how to use the controls in general and what the goals are, therefore requiring no external knowledge before starting the game.</p>
<p>In Portal 2, players are test subjects that have to finish experimental test chambers. Since my project is used for a study and the players are study participants, I wanted to double down on this thematically. You can find a screenshot of the concept starting area down below. Spoiler: Since the player is not particulary safe in Portal 2 either, this starting area might have catched fire and exploded eventually, destroying the starting area and allowing the player to start the parkour.</p>
<p><img src="../../../assets/2024/ivar/prototype_start_area.jpg" alt="An image that shows a prototype of a starting area." title="An image that shows a prototype of a starting area." /></p>
<h2>Conclusion</h2>
<p>During this project I was able to learn a lot about the Unity Engine and VR. I designed and executed an in-lab user study with participants and analysed the results, something that I had only done once before, but which is something that I find quite interesting. Figuring out the physiological and psychological reasons, why humans interact with computer systems in the way they do and trying to understand what methods work under which particular conditions can be quite exciting.</p>
<h2>Resources</h2>
<ul>
<li><a href="https://github.com/Croydon/tuda-vr-parkour">Implementation Repository</a><ul>
<li><a href="https://github.com/Croydon/tuda-vr-parkour/releases">APK Downloads for Meta Quest 2</a></li>
</ul>
</li>
<li><a href="https://github.com/Croydon/tuda-vr-parkour-study">Study Data Repository</a></li>
<li>Videos<ul>
<li><a href="https://www.youtube.com/watch?v=WJ9H1r-tjjA">Study Task Walkthrough</a></li>
<li><a href="https://www.youtube.com/watch?v=aYS97-tJnGo">Portals Showcase</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL2N-uWvcQaWtL2H0RFa1sC37JADNsBbkO">Video Playlist of all Recorded Participant Tasks</a></li>
</ul>
</li>
<li><a href="https://github.com/wenjietseng/VR-locomotion-parkour">Original Parkour Implementation</a></li>
</ul>
					</div>
				</div>
			</div>
		</div>
	</div>

	<div class="fh5co-navigation">
		<div class="fh5co-cover prev fh5co-cover-sm" style="background-image: url(../../../assets/2020/rss/rss.png)">
			<div class="overlay"></div>

			<a class="copy" href="../../../2020/07/rss-feed-now-available/">
				<div class="display-t">
					<div class="display-tc">
						<div>
							<span>Previous Post</span>
							<h2>RSS Feed Now Available</h2>
						</div>
					</div>
				</div>
			</a>
		</div>
		<div class="fh5co-cover prev fh5co-cover-sm">
			<div class="overlay"></div>
			<span class="copy">
				<div class="display-t">
				</div>
			</span>
		</div>
	</div>
	<footer>
		<div>
			Â© Cr0ydon | Design elements based on <a href="https://demos.freehtml5.co/story/" target="_blank">FreeHTML5.co/story</a> and <a href="https://besrourms.github.io" target="_blank">Mohamed Safouan Besrour</a>'s work.
		</div>
	</footer>

	<div class="gototop js-top">
		<a href="index.html#" class="js-gotop"><i class="bi bi-arrow-up" aria-label="Go To Top Of Page"></i></a>
	</div>
	
	<!-- jQuery -->
	<script src="../../../themes/historia/js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script defer src="../../../themes/historia/js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script defer src="../../../themes/historia/js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script defer src="../../../themes/historia/js/jquery.waypoints.min.js"></script>
	<!-- Stellar Parallax -->
	<script src="../../../themes/historia/js/jquery.stellar.min.js"></script>
	<!-- Main -->
	<script src="../../../themes/historia/js/main.js"></script>

	</body>
</html>
